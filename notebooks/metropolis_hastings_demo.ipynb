{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-cell",
   "metadata": {},
   "source": [
    "# Metropolis-Hastings MCMC: A Comprehensive Tutorial\n",
    "\n",
    "The Metropolis-Hastings algorithm is a powerful Markov Chain Monte Carlo (MCMC) method for sampling from complex probability distributions. Unlike specialized methods like Gibbs sampling, Metropolis-Hastings is extremely general - you only need to evaluate the target density up to a proportionality constant.\n",
    "\n",
    "## Algorithm Overview\n",
    "\n",
    "Given a target distribution π(x) that we want to sample from, Metropolis-Hastings works by:\n",
    "\n",
    "1. **Proposal**: From current state x_t, propose a new state x' using proposal distribution q(x'|x_t)\n",
    "2. **Accept/Reject**: Accept x' with probability α = min(1, [π(x') × q(x_t|x')] / [π(x_t) × q(x'|x_t)])\n",
    "3. **Update**: Set x_{t+1} = x' if accepted, otherwise x_{t+1} = x_t\n",
    "\n",
    "For **Random Walk Metropolis** (our focus), the proposal is symmetric: x' = x_t + ε where ε ~ N(0, σ²I). This simplifies the acceptance probability to α = min(1, π(x')/π(x_t)).\n",
    "\n",
    "This notebook demonstrates the algorithm with clear examples, explores the trade-offs in proposal tuning, and shows how to use ArviZ for comprehensive MCMC diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from mc_lab.metropolis_hastings import MetropolisHastingsSampler\n",
    "\n",
    "# Configuration\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "\n",
    "# Plotting configuration\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"ArviZ: {az.__version__}\")\n",
    "print(f\"Random seed: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-example-header",
   "metadata": {},
   "source": [
    "## Example 1: Sampling from a 1D Normal Distribution\n",
    "\n",
    "Let's start with the simplest case - sampling from a standard normal distribution N(0,1). This helps us understand the basic mechanics without distractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-example-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_standard_normal(x: float) -> float:\n",
    "    \"\"\"Log probability density of standard normal N(0,1).\"\"\"\n",
    "    return -0.5 * x**2 - 0.5 * np.log(2 * np.pi)\n",
    "\n",
    "# Create sampler with moderate proposal scale\n",
    "sampler = MetropolisHastingsSampler(\n",
    "    log_target=log_standard_normal,\n",
    "    proposal_scale=1.0,\n",
    "    adaptive_scaling=True,\n",
    "    var_names=[\"x\"]\n",
    ")\n",
    "\n",
    "# Generate samples\n",
    "print(\"Sampling from standard normal distribution...\")\n",
    "idata = sampler.sample(\n",
    "    n_samples=2000,\n",
    "    n_chains=3,\n",
    "    burn_in=500,\n",
    "    thin=1,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "print(\"\\nSampling complete!\")\n",
    "print(f\"Posterior shape: {idata.posterior.x.shape} (chains × samples)\")\n",
    "\n",
    "# Check acceptance rates\n",
    "acceptance_rates = sampler.get_acceptance_rates(idata)\n",
    "print(f\"\\nAcceptance rates:\")\n",
    "for chain, rate in acceptance_rates.items():\n",
    "    print(f\"  {chain}: {rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-diagnostics-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical validation\n",
    "samples = idata.posterior.x.values.flatten()\n",
    "sample_mean = np.mean(samples)\n",
    "sample_std = np.std(samples, ddof=1)\n",
    "\n",
    "print(f\"Sample statistics:\")\n",
    "print(f\"  Mean: {sample_mean:.4f} (true: 0.0000)\")\n",
    "print(f\"  Std:  {sample_std:.4f} (true: 1.0000)\")\n",
    "print(f\"  Min:  {np.min(samples):.4f}\")\n",
    "print(f\"  Max:  {np.max(samples):.4f}\")\n",
    "\n",
    "# Kolmogorov-Smirnov test against true distribution\n",
    "ks_stat, p_value = stats.kstest(samples, stats.norm.cdf)\n",
    "print(f\"\\nKS test vs N(0,1): statistic={ks_stat:.4f}, p-value={p_value:.4f}\")\n",
    "if p_value > 0.05:\n",
    "    print(\"✓ Samples are consistent with target distribution\")\n",
    "else:\n",
    "    print(\"⚠ Samples may not match target distribution well\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-plots-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Trace plot\n",
    "ax = axes[0, 0]\n",
    "for chain in range(idata.posterior.x.shape[0]):\n",
    "    ax.plot(idata.posterior.x[chain], alpha=0.8, label=f\"Chain {chain+1}\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"x\")\n",
    "ax.set_title(\"Trace Plot\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Sample histogram vs true density\n",
    "ax = axes[0, 1]\n",
    "ax.hist(samples, bins=50, density=True, alpha=0.7, label=\"Samples\", color=\"skyblue\")\n",
    "x_true = np.linspace(-4, 4, 100)\n",
    "y_true = stats.norm.pdf(x_true)\n",
    "ax.plot(x_true, y_true, 'r-', linewidth=2, label=\"True N(0,1)\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"Sample Distribution\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Acceptance rate over time\n",
    "ax = axes[1, 0]\n",
    "accepted = idata.sample_stats.accepted.values\n",
    "window = 100\n",
    "for chain in range(accepted.shape[0]):\n",
    "    # Rolling acceptance rate\n",
    "    rolling_accept = np.convolve(accepted[chain].astype(float), \n",
    "                                np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(accepted[chain])), rolling_accept, \n",
    "            alpha=0.8, label=f\"Chain {chain+1}\")\n",
    "ax.axhline(y=0.35, color='red', linestyle='--', alpha=0.7, label='Target (35%)')\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Acceptance Rate\")\n",
    "ax.set_title(f\"Rolling Acceptance Rate (window={window})\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Proposal scale adaptation\n",
    "ax = axes[1, 1]\n",
    "proposal_scales = idata.sample_stats.proposal_scale.values\n",
    "for chain in range(proposal_scales.shape[0]):\n",
    "    ax.plot(proposal_scales[chain, :, 0], alpha=0.8, label=f\"Chain {chain+1}\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Proposal Scale\")\n",
    "ax.set_title(\"Proposal Scale Adaptation\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arviz-diagnostics-header",
   "metadata": {},
   "source": [
    "## ArviZ MCMC Diagnostics\n",
    "\n",
    "ArviZ provides a comprehensive suite of MCMC diagnostics. Let's explore the most important ones for assessing chain convergence and sample quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arviz-summary-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive summary statistics\n",
    "print(\"=== ArviZ Summary ===\")\n",
    "summary = az.summary(idata, round_to=4)\n",
    "print(summary)\n",
    "\n",
    "print(\"\\n=== Key Diagnostics ===\")\n",
    "# R-hat (should be < 1.01 for convergence)\n",
    "rhat = az.rhat(idata)\n",
    "print(f\"R-hat: {rhat.x.values:.4f} (< 1.01 is good)\")\n",
    "\n",
    "# Effective Sample Size\n",
    "ess_bulk = az.ess(idata, method=\"bulk\")\n",
    "ess_tail = az.ess(idata, method=\"tail\")\n",
    "print(f\"ESS (bulk): {ess_bulk.x.values:.0f} (> 400 recommended)\")\n",
    "print(f\"ESS (tail): {ess_tail.x.values:.0f} (> 400 recommended)\")\n",
    "\n",
    "# Monte Carlo Standard Error\n",
    "mcse_mean = az.mcse(idata, method=\"mean\")\n",
    "mcse_sd = az.mcse(idata, method=\"sd\")\n",
    "print(f\"MCSE (mean): {mcse_mean.x.values:.4f}\")\n",
    "print(f\"MCSE (sd): {mcse_sd.x.values:.4f}\")\n",
    "\n",
    "print(\"\\n=== Interpretation ===\")\n",
    "if rhat.x.values < 1.01:\n",
    "    print(\"✓ R-hat indicates good convergence\")\n",
    "else:\n",
    "    print(\"⚠ R-hat suggests convergence issues - run longer chains\")\n",
    "    \n",
    "if ess_bulk.x.values > 400:\n",
    "    print(\"✓ Effective sample size is adequate for mean estimation\")\n",
    "else:\n",
    "    print(\"⚠ Low effective sample size - may need more samples or better mixing\")\n",
    "    \n",
    "if ess_tail.x.values > 400:\n",
    "    print(\"✓ Effective sample size is adequate for tail quantile estimation\")\n",
    "else:\n",
    "    print(\"⚠ Low tail ESS - quantile estimates may be unreliable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arviz-plots-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ArviZ diagnostic plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Trace plot with ArviZ\n",
    "az.plot_trace(idata, var_names=[\"x\"], ax=axes[0, :], combined=False)\n",
    "axes[0, 0].set_title(\"Trace Plot (ArviZ)\")\n",
    "axes[0, 1].set_title(\"Marginal Distribution\")\n",
    "\n",
    "# Autocorrelation\n",
    "az.plot_autocorr(idata, var_names=[\"x\"], ax=axes[1, 0], combined=False)\n",
    "axes[1, 0].set_title(\"Autocorrelation Function\")\n",
    "\n",
    "# Rank plot\n",
    "az.plot_rank(idata, var_names=[\"x\"], ax=axes[1, 1])\n",
    "axes[1, 1].set_title(\"Rank Plot\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ESS evolution plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "az.plot_ess(idata, var_names=[\"x\"], kind=\"evolution\")\n",
    "plt.title(\"Effective Sample Size Evolution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proposal-tuning-header",
   "metadata": {},
   "source": [
    "## Example 2: The Importance of Proposal Tuning\n",
    "\n",
    "The choice of proposal scale σ is crucial for efficient sampling. Too small → high acceptance but poor mixing. Too large → low acceptance and wasted computation. Let's explore this trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proposal-tuning-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_proposal_scales(target_log_pdf, scales, n_samples=1000, seed=None):\n",
    "    \"\"\"Compare different proposal scales for the same target distribution.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for i, scale in enumerate(scales):\n",
    "        print(f\"\\nTesting proposal scale: {scale}\")\n",
    "        \n",
    "        sampler = MetropolisHastingsSampler(\n",
    "            log_target=target_log_pdf,\n",
    "            proposal_scale=scale,\n",
    "            adaptive_scaling=False,  # Fixed scale for fair comparison\n",
    "        )\n",
    "        \n",
    "        idata = sampler.sample(\n",
    "            n_samples=n_samples,\n",
    "            n_chains=1,\n",
    "            burn_in=200,\n",
    "            random_seed=seed + i if seed else None,\n",
    "            progressbar=False,\n",
    "        )\n",
    "        \n",
    "        # Extract results\n",
    "        samples = idata.posterior.x.values.flatten()\n",
    "        acceptance_rate = sampler.get_acceptance_rates(idata)[\"overall\"]\n",
    "        ess = az.ess(idata).x.values\n",
    "        \n",
    "        results[scale] = {\n",
    "            'samples': samples,\n",
    "            'acceptance_rate': acceptance_rate,\n",
    "            'ess': ess,\n",
    "            'ess_per_sample': ess / n_samples,\n",
    "            'sample_var': np.var(samples, ddof=1)\n",
    "        }\n",
    "        \n",
    "        print(f\"  Acceptance rate: {acceptance_rate:.3f}\")\n",
    "        print(f\"  ESS: {ess:.0f} ({ess/n_samples:.3f} efficiency)\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Test different proposal scales\n",
    "scales_to_test = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "print(\"Comparing proposal scales for standard normal target...\")\n",
    "\n",
    "results = compare_proposal_scales(\n",
    "    target_log_pdf=log_standard_normal,\n",
    "    scales=scales_to_test,\n",
    "    n_samples=2000,\n",
    "    seed=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proposal-comparison-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trade-offs\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Extract data for plotting\n",
    "scales = list(results.keys())\n",
    "accept_rates = [results[s]['acceptance_rate'] for s in scales]\n",
    "ess_values = [results[s]['ess'] for s in scales]\n",
    "ess_efficiency = [results[s]['ess_per_sample'] for s in scales]\n",
    "\n",
    "# Trade-off plot: Acceptance vs ESS\n",
    "ax = axes[0, 0]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(scales)))\n",
    "scatter = ax.scatter(accept_rates, ess_values, c=scales, cmap='viridis', s=100, alpha=0.8)\n",
    "for i, scale in enumerate(scales):\n",
    "    ax.annotate(f'σ={scale}', (accept_rates[i], ess_values[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "ax.set_xlabel('Acceptance Rate')\n",
    "ax.set_ylabel('Effective Sample Size')\n",
    "ax.set_title('Acceptance Rate vs ESS Trade-off')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax, label='Proposal Scale')\n",
    "\n",
    "# Efficiency plot\n",
    "ax = axes[0, 1]\n",
    "ax.plot(scales, ess_efficiency, 'o-', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Proposal Scale')\n",
    "ax.set_ylabel('ESS / Sample (Efficiency)')\n",
    "ax.set_title('Sampling Efficiency vs Proposal Scale')\n",
    "ax.grid(True, alpha=0.3)\n",
    "# Mark the best efficiency\n",
    "best_idx = np.argmax(ess_efficiency)\n",
    "ax.axvline(scales[best_idx], color='red', linestyle='--', alpha=0.7, \n",
    "           label=f'Best: σ={scales[best_idx]}')\n",
    "ax.legend()\n",
    "\n",
    "# Acceptance rate plot\n",
    "ax = axes[0, 2]\n",
    "ax.plot(scales, accept_rates, 's-', linewidth=2, markersize=8, color='orange')\n",
    "ax.axhline(y=0.35, color='red', linestyle='--', alpha=0.7, label='Theoretical Optimum')\n",
    "ax.set_xlabel('Proposal Scale')\n",
    "ax.set_ylabel('Acceptance Rate')\n",
    "ax.set_title('Acceptance Rate vs Proposal Scale')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "# Sample traces for extreme cases\n",
    "extreme_scales = [0.1, 2.0]  # Too small, good scale\n",
    "for i, scale in enumerate(extreme_scales):\n",
    "    ax = axes[1, i]\n",
    "    trace = results[scale]['samples'][:500]  # First 500 samples\n",
    "    ax.plot(trace, alpha=0.8)\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('x')\n",
    "    ax.set_title(f'Trace: σ={scale} (Accept: {results[scale][\"acceptance_rate\"]:.3f})')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add interpretation\n",
    "    if scale == 0.1:\n",
    "        ax.text(0.05, 0.95, 'Too small: High acceptance\\nbut poor mixing', \n",
    "                transform=ax.transAxes, va='top', bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "    else:\n",
    "        ax.text(0.05, 0.95, 'Good scale: Balanced\\nacceptance and mixing', \n",
    "                transform=ax.transAxes, va='top', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "# Summary table\n",
    "ax = axes[1, 2]\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table_data = []\n",
    "for scale in scales:\n",
    "    r = results[scale]\n",
    "    table_data.append([f'{scale}', f'{r[\"acceptance_rate\"]:.3f}', \n",
    "                      f'{r[\"ess\"]:.0f}', f'{r[\"ess_per_sample\"]:.3f}'])\n",
    "\n",
    "table = ax.table(cellText=table_data, \n",
    "                colLabels=['Scale σ', 'Accept Rate', 'ESS', 'Efficiency'],\n",
    "                cellLoc='center', loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "ax.set_title('Summary Statistics', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== Key Insights ===\")\n",
    "print(f\"Best efficiency at σ={scales[best_idx]} with {ess_efficiency[best_idx]:.3f} ESS/sample\")\n",
    "print(f\"Theoretical optimal acceptance rate: ~0.35 for 1D problems\")\n",
    "print(f\"Closest to optimal: σ={scales[np.argmin(np.abs(np.array(accept_rates) - 0.35))]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multivariate-header",
   "metadata": {},
   "source": [
    "## Example 3: 2D Multivariate Normal Distribution\n",
    "\n",
    "Let's move to a more complex example - a 2D correlated normal distribution. This shows how the sampler handles multivariate targets and correlation structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multivariate-setup-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 2D correlated normal distribution\n",
    "true_mean = np.array([2.0, -1.0])\n",
    "true_cov = np.array([[1.5, 0.8], [0.8, 2.0]])\n",
    "cov_inv = np.linalg.inv(true_cov)\n",
    "log_det_cov = np.log(np.linalg.det(true_cov))\n",
    "\n",
    "def log_mvn_2d(x):\n",
    "    \"\"\"Log PDF of 2D multivariate normal distribution.\"\"\"\n",
    "    x = np.atleast_1d(x)\n",
    "    diff = x - true_mean\n",
    "    return -0.5 * (diff @ cov_inv @ diff + log_det_cov + 2 * np.log(2 * np.pi))\n",
    "\n",
    "print(\"2D Multivariate Normal Distribution:\")\n",
    "print(f\"Mean: {true_mean}\")\n",
    "print(f\"Covariance:\")\n",
    "print(true_cov)\n",
    "print(f\"Correlation: {true_cov[0,1] / np.sqrt(true_cov[0,0] * true_cov[1,1]):.3f}\")\n",
    "\n",
    "# Create sampler with dimension-specific proposal scales\n",
    "sampler_2d = MetropolisHastingsSampler(\n",
    "    log_target=log_mvn_2d,\n",
    "    proposal_scale=np.array([0.8, 0.9]),  # Different scales for each dimension\n",
    "    var_names=[\"theta1\", \"theta2\"],\n",
    "    adaptive_scaling=True,\n",
    ")\n",
    "\n",
    "# Generate samples\n",
    "print(\"\\nSampling from 2D multivariate normal...\")\n",
    "idata_2d = sampler_2d.sample(\n",
    "    n_samples=3000,\n",
    "    n_chains=4,\n",
    "    burn_in=1000,\n",
    "    thin=1,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "print(\"\\nSampling complete!\")\n",
    "print(f\"Shape - theta1: {idata_2d.posterior.theta1.shape}\")\n",
    "print(f\"Shape - theta2: {idata_2d.posterior.theta2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multivariate-analysis-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and analyze samples\n",
    "samples_theta1 = idata_2d.posterior.theta1.values.flatten()\n",
    "samples_theta2 = idata_2d.posterior.theta2.values.flatten()\n",
    "samples_2d = np.column_stack([samples_theta1, samples_theta2])\n",
    "\n",
    "# Compute sample statistics\n",
    "sample_mean_2d = np.mean(samples_2d, axis=0)\n",
    "sample_cov_2d = np.cov(samples_2d.T)\n",
    "sample_corr = sample_cov_2d[0, 1] / np.sqrt(sample_cov_2d[0, 0] * sample_cov_2d[1, 1])\n",
    "true_corr = true_cov[0, 1] / np.sqrt(true_cov[0, 0] * true_cov[1, 1])\n",
    "\n",
    "print(\"=== 2D Multivariate Normal Results ===\")\n",
    "print(f\"\\nMeans:\")\n",
    "print(f\"  Sample: [{sample_mean_2d[0]:.3f}, {sample_mean_2d[1]:.3f}]\")\n",
    "print(f\"  True:   [{true_mean[0]:.3f}, {true_mean[1]:.3f}]\")\n",
    "print(f\"  Error:  [{abs(sample_mean_2d[0] - true_mean[0]):.3f}, {abs(sample_mean_2d[1] - true_mean[1]):.3f}]\")\n",
    "\n",
    "print(f\"\\nCovariances:\")\n",
    "print(f\"Sample:\")\n",
    "print(f\"  [{sample_cov_2d[0,0]:.3f}, {sample_cov_2d[0,1]:.3f}]\")\n",
    "print(f\"  [{sample_cov_2d[1,0]:.3f}, {sample_cov_2d[1,1]:.3f}]\")\n",
    "print(f\"True:\")\n",
    "print(f\"  [{true_cov[0,0]:.3f}, {true_cov[0,1]:.3f}]\")\n",
    "print(f\"  [{true_cov[1,0]:.3f}, {true_cov[1,1]:.3f}]\")\n",
    "\n",
    "print(f\"\\nCorrelation:\")\n",
    "print(f\"  Sample: {sample_corr:.4f}\")\n",
    "print(f\"  True:   {true_corr:.4f}\")\n",
    "print(f\"  Error:  {abs(sample_corr - true_corr):.4f}\")\n",
    "\n",
    "# Acceptance rates by chain\n",
    "acceptance_rates_2d = sampler_2d.get_acceptance_rates(idata_2d)\n",
    "print(f\"\\nAcceptance rates:\")\n",
    "for chain, rate in acceptance_rates_2d.items():\n",
    "    print(f\"  {chain}: {rate:.3f}\")\n",
    "\n",
    "# ArviZ summary\n",
    "print(f\"\\n=== ArviZ Diagnostics ===\")\n",
    "rhat_2d = az.rhat(idata_2d)\n",
    "ess_2d = az.ess(idata_2d)\n",
    "print(f\"R-hat - theta1: {rhat_2d.theta1.values:.4f}, theta2: {rhat_2d.theta2.values:.4f}\")\n",
    "print(f\"ESS   - theta1: {ess_2d.theta1.values:.0f}, theta2: {ess_2d.theta2.values:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multivariate-plots-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive 2D visualization\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Joint scatter plot with true distribution contours\n",
    "ax_main = fig.add_subplot(gs[:2, :2])\n",
    "\n",
    "# Sample scatter plot\n",
    "ax_main.scatter(samples_theta1, samples_theta2, alpha=0.3, s=1, c='blue', label='Samples')\n",
    "\n",
    "# True distribution contours\n",
    "x_range = np.linspace(samples_theta1.min() - 0.5, samples_theta1.max() + 0.5, 100)\n",
    "y_range = np.linspace(samples_theta2.min() - 0.5, samples_theta2.max() + 0.5, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "pos = np.dstack((X, Y))\n",
    "\n",
    "# True density for contours\n",
    "true_density = stats.multivariate_normal(true_mean, true_cov)\n",
    "ax_main.contour(X, Y, true_density.pdf(pos), levels=5, colors='red', alpha=0.8)\n",
    "ax_main.contour(X, Y, true_density.pdf(pos), levels=[0.05], colors='red', \n",
    "               linewidths=2, linestyles='--', alpha=0.8)\n",
    "\n",
    "# Mark true and sample means\n",
    "ax_main.scatter(*true_mean, color='red', s=100, marker='x', linewidth=3, label='True Mean')\n",
    "ax_main.scatter(*sample_mean_2d, color='orange', s=100, marker='+', linewidth=3, label='Sample Mean')\n",
    "\n",
    "ax_main.set_xlabel('theta1')\n",
    "ax_main.set_ylabel('theta2')\n",
    "ax_main.set_title('2D Sample Distribution with True Contours')\n",
    "ax_main.legend()\n",
    "ax_main.grid(True, alpha=0.3)\n",
    "\n",
    "# Marginal histogram - theta1\n",
    "ax_marg_x = fig.add_subplot(gs[2, :2])\n",
    "ax_marg_x.hist(samples_theta1, bins=50, density=True, alpha=0.7, color='blue', label='Samples')\n",
    "# True marginal\n",
    "x_true = np.linspace(samples_theta1.min(), samples_theta1.max(), 100)\n",
    "true_marg_1 = stats.norm.pdf(x_true, true_mean[0], np.sqrt(true_cov[0, 0]))\n",
    "ax_marg_x.plot(x_true, true_marg_1, 'r-', linewidth=2, label='True Marginal')\n",
    "ax_marg_x.set_xlabel('theta1')\n",
    "ax_marg_x.set_ylabel('Density')\n",
    "ax_marg_x.set_title('theta1 Marginal Distribution')\n",
    "ax_marg_x.legend()\n",
    "ax_marg_x.grid(True, alpha=0.3)\n",
    "\n",
    "# Marginal histogram - theta2\n",
    "ax_marg_y = fig.add_subplot(gs[:2, 2])\n",
    "ax_marg_y.hist(samples_theta2, bins=50, density=True, alpha=0.7, color='blue', orientation='horizontal')\n",
    "# True marginal\n",
    "y_true = np.linspace(samples_theta2.min(), samples_theta2.max(), 100)\n",
    "true_marg_2 = stats.norm.pdf(y_true, true_mean[1], np.sqrt(true_cov[1, 1]))\n",
    "ax_marg_y.plot(true_marg_2, y_true, 'r-', linewidth=2)\n",
    "ax_marg_y.set_ylabel('theta2')\n",
    "ax_marg_y.set_xlabel('Density')\n",
    "ax_marg_y.set_title('theta2 Marginal Distribution')\n",
    "ax_marg_y.grid(True, alpha=0.3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ArviZ-specific plots\n",
    "print(\"\\n=== ArviZ Diagnostic Plots ===\")\n",
    "\n",
    "# Trace plot\n",
    "az.plot_trace(idata_2d, figsize=(14, 6))\n",
    "plt.suptitle('2D MCMC Trace Plots', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Pair plot\n",
    "az.plot_pair(idata_2d, figsize=(10, 8), kind=['scatter', 'kde'], marginals=True)\n",
    "plt.suptitle('Pair Plot with Marginals', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Posterior distributions\n",
    "az.plot_posterior(idata_2d, figsize=(12, 4), ref_val={'theta1': true_mean[0], 'theta2': true_mean[1]})\n",
    "plt.suptitle('Posterior Distributions with True Values', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-header",
   "metadata": {},
   "source": [
    "## Example 4: Challenging Distribution - Banana-Shaped\n",
    "\n",
    "Let's test the sampler on a more challenging target: a \"banana-shaped\" distribution. This tests the algorithm's ability to handle strong correlations and nonlinear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banana-setup-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_banana_distribution(x, a=1.0, b=100.0):\n",
    "    \"\"\"\n",
    "    Log density of the famous 'banana' distribution.\n",
    "    \n",
    "    This is a transformed 2D normal where:\n",
    "    - x1 ~ N(0, a²)\n",
    "    - x2 | x1 ~ N(b*x1², 1)\n",
    "    \n",
    "    The result is a curved, banana-shaped distribution that's \n",
    "    challenging for MCMC due to strong nonlinear correlation.\n",
    "    \"\"\"\n",
    "    x = np.atleast_1d(x)\n",
    "    if len(x) != 2:\n",
    "        raise ValueError(\"Banana distribution requires 2D input\")\n",
    "        \n",
    "    x1, x2 = x[0], x[1]\n",
    "    \n",
    "    # Log density components\n",
    "    log_p_x1 = -0.5 * (x1**2) / (a**2) - 0.5 * np.log(2 * np.pi * a**2)\n",
    "    log_p_x2_given_x1 = -0.5 * (x2 - b * x1**2)**2 - 0.5 * np.log(2 * np.pi)\n",
    "    \n",
    "    return log_p_x1 + log_p_x2_given_x1\n",
    "\n",
    "# Create banana sampler\n",
    "print(\"Setting up banana distribution sampler...\")\n",
    "print(\"This is a challenging nonlinear target with strong correlation.\")\n",
    "\n",
    "sampler_banana = MetropolisHastingsSampler(\n",
    "    log_target=lambda x: log_banana_distribution(x, a=2.0, b=0.5),\n",
    "    proposal_scale=np.array([0.5, 0.8]),  # May need tuning\n",
    "    var_names=[\"x1\", \"x2\"],\n",
    "    adaptive_scaling=True,\n",
    "    target_acceptance_rate=0.3,  # Slightly lower for challenging target\n",
    ")\n",
    "\n",
    "# Generate samples (need more for challenging distribution)\n",
    "print(\"\\nSampling from banana distribution...\")\n",
    "print(\"This may take longer due to the challenging geometry.\")\n",
    "\n",
    "idata_banana = sampler_banana.sample(\n",
    "    n_samples=4000,\n",
    "    n_chains=4,\n",
    "    burn_in=2000,  # Longer burn-in for challenging target\n",
    "    thin=2,        # More thinning to reduce correlation\n",
    "    initial_states=np.array([[-1, 0], [1, 0], [0, -2], [0, 2]]),  # Overdispersed starts\n",
    "    random_seed=RANDOM_SEED,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "print(\"\\nBanana sampling complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banana-analysis-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze banana distribution results\n",
    "samples_x1 = idata_banana.posterior.x1.values.flatten()\n",
    "samples_x2 = idata_banana.posterior.x2.values.flatten()\n",
    "samples_banana = np.column_stack([samples_x1, samples_x2])\n",
    "\n",
    "print(\"=== Banana Distribution Results ===\")\n",
    "print(f\"Total samples: {len(samples_x1)}\")\n",
    "print(f\"Sample means: x1={np.mean(samples_x1):.3f}, x2={np.mean(samples_x2):.3f}\")\n",
    "print(f\"Sample stds:  x1={np.std(samples_x1):.3f}, x2={np.std(samples_x2):.3f}\")\n",
    "\n",
    "# Acceptance rates\n",
    "banana_acceptance = sampler_banana.get_acceptance_rates(idata_banana)\n",
    "print(f\"\\nAcceptance rates:\")\n",
    "for chain, rate in banana_acceptance.items():\n",
    "    print(f\"  {chain}: {rate:.3f}\")\n",
    "\n",
    "# Convergence diagnostics\n",
    "rhat_banana = az.rhat(idata_banana)\n",
    "ess_banana = az.ess(idata_banana)\n",
    "\n",
    "print(f\"\\n=== Convergence Diagnostics ===\")\n",
    "print(f\"R-hat - x1: {rhat_banana.x1.values:.4f}, x2: {rhat_banana.x2.values:.4f}\")\n",
    "print(f\"ESS   - x1: {ess_banana.x1.values:.0f}, x2: {ess_banana.x2.values:.0f}\")\n",
    "\n",
    "# Check for convergence issues\n",
    "convergence_ok = True\n",
    "if rhat_banana.x1.values > 1.01 or rhat_banana.x2.values > 1.01:\n",
    "    print(\"⚠ R-hat indicates potential convergence issues\")\n",
    "    convergence_ok = False\n",
    "\n",
    "if ess_banana.x1.values < 400 or ess_banana.x2.values < 400:\n",
    "    print(\"⚠ Low effective sample size - consider longer runs\")\n",
    "    convergence_ok = False\n",
    "    \n",
    "if convergence_ok:\n",
    "    print(\"✓ Convergence diagnostics look good!\")\n",
    "else:\n",
    "    print(\"  This is expected for challenging distributions.\")\n",
    "    print(\"  In practice, you might need longer chains or better proposals.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banana-visualization-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the banana distribution\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Main scatter plot\n",
    "ax = axes[0, 0]\n",
    "colors = plt.cm.tab10(np.arange(4))\n",
    "for chain in range(4):\n",
    "    chain_x1 = idata_banana.posterior.x1.values[chain]\n",
    "    chain_x2 = idata_banana.posterior.x2.values[chain]\n",
    "    ax.scatter(chain_x1, chain_x2, alpha=0.5, s=2, color=colors[chain], \n",
    "               label=f'Chain {chain+1}')\n",
    "\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_title('Banana Distribution Samples (Colored by Chain)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Density contours\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(samples_x1, samples_x2, alpha=0.3, s=1, c='blue')\n",
    "\n",
    "# Generate true banana contours\n",
    "x1_range = np.linspace(samples_x1.min() - 0.5, samples_x1.max() + 0.5, 50)\n",
    "x2_range = np.linspace(samples_x2.min() - 1, samples_x2.max() + 1, 50)\n",
    "X1_grid, X2_grid = np.meshgrid(x1_range, x2_range)\n",
    "\n",
    "# Compute log densities on grid\n",
    "log_densities = np.zeros_like(X1_grid)\n",
    "for i in range(X1_grid.shape[0]):\n",
    "    for j in range(X1_grid.shape[1]):\n",
    "        log_densities[i, j] = log_banana_distribution([X1_grid[i, j], X2_grid[i, j]], a=2.0, b=0.5)\n",
    "\n",
    "densities = np.exp(log_densities)\n",
    "ax.contour(X1_grid, X2_grid, densities, levels=8, colors='red', alpha=0.8)\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_title('Banana Shape with True Contours')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Trace plots\n",
    "ax = axes[0, 2]\n",
    "for chain in range(4):\n",
    "    ax.plot(idata_banana.posterior.x1.values[chain][:500], alpha=0.7, \n",
    "            color=colors[chain], label=f'Chain {chain+1}' if chain == 0 else \"\")\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('x1')\n",
    "ax.set_title('x1 Trace Plot (First 500 samples)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# x2 trace\n",
    "ax = axes[1, 0]\n",
    "for chain in range(4):\n",
    "    ax.plot(idata_banana.posterior.x2.values[chain][:500], alpha=0.7, color=colors[chain])\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_title('x2 Trace Plot (First 500 samples)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Marginal distributions\n",
    "ax = axes[1, 1]\n",
    "ax.hist(samples_x1, bins=50, density=True, alpha=0.7, color='blue', label='x1 samples')\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('x1 Marginal Distribution')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1, 2]\n",
    "ax.hist(samples_x2, bins=50, density=True, alpha=0.7, color='orange', label='x2 samples')\n",
    "ax.set_xlabel('x2')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('x2 Marginal Distribution')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ArviZ diagnostic plots for banana\n",
    "print(\"\\n=== ArviZ Diagnostics for Banana Distribution ===\")\n",
    "\n",
    "# Trace plot\n",
    "az.plot_trace(idata_banana, figsize=(14, 6))\n",
    "plt.suptitle('Banana Distribution - Trace Plots', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Autocorrelation\n",
    "az.plot_autocorr(idata_banana, figsize=(12, 4), combined=False, max_lag=100)\n",
    "plt.suptitle('Autocorrelation Functions', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Key Observations ===\")\n",
    "print(\"• The banana shape creates strong nonlinear correlation\")\n",
    "print(\"• Trace plots may show slower mixing due to the curved geometry\")\n",
    "print(\"• Autocorrelation tends to be higher for challenging targets\")\n",
    "print(\"• Multiple chains help detect convergence issues\")\n",
    "print(\"• This demonstrates why adaptive proposal scaling is important\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-tips-header",
   "metadata": {},
   "source": [
    "## Practical Tips and Best Practices\n",
    "\n",
    "Based on our examples, here are key guidelines for successful Metropolis-Hastings sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-tips-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_best_practices():\n",
    "    \"\"\"Demonstrate key best practices with examples.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"METROPOLIS-HASTINGS BEST PRACTICES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\n1. PROPOSAL SCALE TUNING\")\n",
    "    print(\"   • Target acceptance rate: 20-50% (theory suggests ~23% for 1D, ~44% for high-D)\")\n",
    "    print(\"   • Too high acceptance (>70%): Steps too small, poor mixing\")\n",
    "    print(\"   • Too low acceptance (<10%): Steps too large, wasted computation\")\n",
    "    print(\"   • Use adaptive scaling during burn-in, then fix for inference\")\n",
    "    \n",
    "    print(\"\\n2. CHAIN INITIALIZATION\")\n",
    "    print(\"   • Use multiple chains (≥4) with overdispersed starting points\")\n",
    "    print(\"   • Starting points should cover the typical set of the posterior\")\n",
    "    print(\"   • Avoid starting all chains at the same point\")\n",
    "    \n",
    "    print(\"\\n3. BURN-IN AND THINNING\")\n",
    "    print(\"   • Burn-in: Discard initial samples while chain reaches stationarity\")\n",
    "    print(\"   • Rule of thumb: 10-50% of total run for burn-in\")\n",
    "    print(\"   • Thinning: Keep every k-th sample to reduce autocorrelation\")\n",
    "    print(\"   • Better to run longer than to thin heavily\")\n",
    "    \n",
    "    print(\"\\n4. CONVERGENCE DIAGNOSTICS\")\n",
    "    print(\"   • R̂ (R-hat): < 1.01 indicates convergence across chains\")\n",
    "    print(\"   • Effective Sample Size (ESS): > 400 for reliable estimates\")\n",
    "    print(\"   • Trace plots: Should look like 'fuzzy caterpillars'\")\n",
    "    print(\"   • Autocorrelation: Should decay to near zero\")\n",
    "    \n",
    "    print(\"\\n5. WHEN TO USE METROPOLIS-HASTINGS\")\n",
    "    print(\"   ✓ General-purpose: Works with any target density\")\n",
    "    print(\"   ✓ Unknown normalizing constants: Only needs density ratios\")\n",
    "    print(\"   ✓ Complex geometries: Can handle multimodal, constrained distributions\")\n",
    "    print(\"   ⚠ May be slow for high-dimensional problems\")\n",
    "    print(\"   ⚠ Requires careful tuning for optimal performance\")\n",
    "    \n",
    "    print(\"\\n6. TROUBLESHOOTING COMMON ISSUES\")\n",
    "    print(\"   Problem: Poor mixing (trace plots show sticking)\")\n",
    "    print(\"   → Solution: Tune proposal scale, try different starting points\")\n",
    "    \n",
    "    print(\"   Problem: High R̂ values\")\n",
    "    print(\"   → Solution: Run longer chains, check for multimodality\")\n",
    "    \n",
    "    print(\"   Problem: Low ESS\")\n",
    "    print(\"   → Solution: Run more samples, reduce autocorrelation\")\n",
    "    \n",
    "    print(\"   Problem: All chains stuck in one mode\")\n",
    "    print(\"   → Solution: More dispersed initialization, longer runs\")\n",
    "\n",
    "demonstrate_best_practices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-table-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison of our examples\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY: COMPARISON OF EXAMPLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "examples = {\n",
    "    \"1D Normal\": {\n",
    "        \"Target\": \"N(0, 1)\",\n",
    "        \"Difficulty\": \"Easy\",\n",
    "        \"Acceptance\": f\"{sampler.get_acceptance_rates(idata)['overall']:.3f}\",\n",
    "        \"R-hat\": f\"{az.rhat(idata).x.values:.4f}\",\n",
    "        \"ESS\": f\"{az.ess(idata).x.values:.0f}\",\n",
    "        \"Key Learning\": \"Basic algorithm, proposal tuning\"\n",
    "    },\n",
    "    \"2D MVN\": {\n",
    "        \"Target\": \"Correlated 2D Normal\",\n",
    "        \"Difficulty\": \"Moderate\",\n",
    "        \"Acceptance\": f\"{sampler_2d.get_acceptance_rates(idata_2d)['overall']:.3f}\",\n",
    "        \"R-hat\": f\"{max(az.rhat(idata_2d).theta1.values, az.rhat(idata_2d).theta2.values):.4f}\",\n",
    "        \"ESS\": f\"{min(az.ess(idata_2d).theta1.values, az.ess(idata_2d).theta2.values):.0f}\",\n",
    "        \"Key Learning\": \"Multivariate sampling, correlation\"\n",
    "    },\n",
    "    \"Banana\": {\n",
    "        \"Target\": \"Nonlinear correlation\",\n",
    "        \"Difficulty\": \"Hard\",\n",
    "        \"Acceptance\": f\"{sampler_banana.get_acceptance_rates(idata_banana)['overall']:.3f}\",\n",
    "        \"R-hat\": f\"{max(az.rhat(idata_banana).x1.values, az.rhat(idata_banana).x2.values):.4f}\",\n",
    "        \"ESS\": f\"{min(az.ess(idata_banana).x1.values, az.ess(idata_banana).x2.values):.0f}\",\n",
    "        \"Key Learning\": \"Challenging geometry, adaptation\"\n",
    "    }\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(examples).T\n",
    "print(df.to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL THOUGHTS\")\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "\"Metropolis-Hastings is a powerful, general-purpose MCMC algorithm that can\\n\"\n",
    "\"sample from virtually any continuous distribution. The key to success is:\\n\\n\"\n",
    "\"1. Proper proposal tuning (acceptance rates 20-50%)\\n\"\n",
    "\"2. Multiple chains with good initialization\\n\"\n",
    "\"3. Adequate burn-in and sample size\\n\"\n",
    "\"4. Rigorous convergence diagnostics with ArviZ\\n\\n\"\n",
    "\"While it may not be the most efficient method for specific problems,\\n\"\n",
    "\"its generality makes it an essential tool in the MCMC toolkit.\\n\\n\"\n",
    "\"For specialized cases (e.g., conjugate models), consider Gibbs sampling.\\n\"\n",
    "\"For high-dimensional problems, explore Hamiltonian Monte Carlo (HMC).\\n\"\n",
    "\"But when you need a reliable, general method, Metropolis-Hastings delivers!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environment-info-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and reproducibility information\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"REPRODUCIBILITY INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"SciPy version: {stats.__version__ if hasattr(stats, '__version__') else 'N/A'}\")\n",
    "print(f\"ArviZ version: {az.__version__}\")\n",
    "print(f\"Matplotlib version: {matplotlib.__version__}\")\n",
    "print(f\"Random seed used: {RANDOM_SEED}\")\n",
    "\n",
    "print(f\"\\nNotebook completed successfully!\")\n",
    "print(f\"All examples demonstrate the Metropolis-Hastings algorithm with\")\n",
    "print(f\"comprehensive ArviZ integration for professional MCMC workflows.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}