{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion-Assisted MCMC Sampling\n",
    "\n",
    "This notebook demonstrates the diffusion-assisted Markov Chain Monte Carlo algorithm from:\n",
    "\n",
    "> Hunt-Smith et al. (2023), \"Accelerating Markov Chain Monte Carlo sampling with diffusion models\", arXiv:2309.01454v1\n",
    "\n",
    "## Key Idea\n",
    "\n",
    "Traditional MCMC methods like Metropolis-Hastings use local proposals (e.g., Gaussian steps from the current point). While this works well for unimodal distributions, it struggles with:\n",
    "\n",
    "1. **Multi-modal distributions**: Hard to jump between distant modes\n",
    "2. **Long-tailed distributions**: Can get stuck far from the mode\n",
    "\n",
    "The solution: **Augment MCMC with a diffusion model** that:\n",
    "- Learns the shape of the posterior from collected samples\n",
    "- Can propose non-local jumps to important regions\n",
    "- Is periodically retrained as sampling progresses\n",
    "- Doesn't require gradient information (works with black-box likelihoods)\n",
    "\n",
    "## Algorithm Overview\n",
    "\n",
    "**Algorithm 1: Diffusion-Assisted Metropolis-Hastings**\n",
    "\n",
    "```\n",
    "for i = 1, ..., n_samples:\n",
    "    if random() < p_diff:\n",
    "        # Global proposal from diffusion model\n",
    "        theta' ~ DiffusionModel()\n",
    "        accept with probability min(1, P(theta') / P(theta) * Q(theta) / Q(theta'))\n",
    "    else:\n",
    "        # Local Gaussian proposal\n",
    "        theta' ~ Normal(theta, sigma^2)\n",
    "        accept with probability min(1, P(theta') / P(theta))\n",
    "    \n",
    "    if i % retrain_interval == 0:\n",
    "        retrain diffusion model on existing samples\n",
    "```\n",
    "\n",
    "The diffusion model learns by:\n",
    "1. **Forward process**: Add Gaussian noise progressively to samples\n",
    "2. **Reverse process**: Learn to denoise by fitting parameters\n",
    "3. **Sampling**: Start from pure noise and apply reverse process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from mc_lab.diffusion_mcmc import diffusion_assisted_mcmc\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Standard Bivariate Gaussian\n",
    "\n",
    "Let's start with a simple 2D Gaussian: $\\theta \\sim N(0, I)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define log-posterior\n",
    "def log_posterior_gaussian(theta):\n",
    "    \"\"\"Log-posterior for standard Gaussian: log P(theta) = -0.5 * ||theta||^2\"\"\"\n",
    "    return -0.5 * np.sum(theta**2)\n",
    "\n",
    "# Sample using diffusion-assisted MCMC\n",
    "initial = np.array([0.0, 0.0])\n",
    "samples, accepted, info = diffusion_assisted_mcmc(\n",
    "    log_posterior_gaussian,\n",
    "    initial,\n",
    "    n_samples=5000,\n",
    "    p_diff=0.3,\n",
    "    sigma_mh=1.0,\n",
    "    retrain_interval=500,\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nOverall acceptance rate: {np.mean(accepted):.1%}\")\n",
    "print(f\"Diffusion acceptance rate: {info['diffusion_acceptance_rate']:.1%}\")\n",
    "print(f\"Gaussian acceptance rate: {info['gaussian_acceptance_rate']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "burn_in = 1000\n",
    "samples_burned = samples[burn_in:]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Trace plots\n",
    "axes[0].plot(samples[:, 0], alpha=0.5, color=\"#117733\")\n",
    "axes[0].axvline(burn_in, color=\"#CC6677\", linestyle=\"--\", label=\"Burn-in\")\n",
    "axes[0].set_xlabel(\"Iteration\")\n",
    "axes[0].set_ylabel(r\"$\\theta_1$\")\n",
    "axes[0].set_title(\"Trace Plot (Dimension 1)\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# 2D scatter\n",
    "axes[1].scatter(samples_burned[:, 0], samples_burned[:, 1], alpha=0.3, s=1, color=\"#44AA99\")\n",
    "axes[1].set_xlabel(r\"$\\theta_1$\")\n",
    "axes[1].set_ylabel(r\"$\\theta_2$\")\n",
    "axes[1].set_title(\"Samples (after burn-in)\")\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].axis(\"equal\")\n",
    "\n",
    "# Marginal distributions\n",
    "axes[2].hist(samples_burned[:, 0], bins=50, alpha=0.6, color=\"#117733\", density=True, label=r\"$\\theta_1$\")\n",
    "axes[2].hist(samples_burned[:, 1], bins=50, alpha=0.6, color=\"#44AA99\", density=True, label=r\"$\\theta_2$\")\n",
    "x = np.linspace(-4, 4, 100)\n",
    "axes[2].plot(x, stats.norm.pdf(x), \"k--\", label=\"True N(0,1)\")\n",
    "axes[2].set_xlabel(r\"$\\theta$\")\n",
    "axes[2].set_ylabel(\"Density\")\n",
    "axes[2].set_title(\"Marginal Distributions\")\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Sample mean: [{np.mean(samples_burned[:, 0]):.3f}, {np.mean(samples_burned[:, 1]):.3f}]\")\n",
    "print(f\"Sample std:  [{np.std(samples_burned[:, 0]):.3f}, {np.std(samples_burned[:, 1]):.3f}]\")\n",
    "print(f\"True mean:   [0.000, 0.000]\")\n",
    "print(f\"True std:    [1.000, 1.000]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Bimodal Distribution\n",
    "\n",
    "Now let's try a more challenging bimodal distribution: a mixture of two Gaussians.\n",
    "\n",
    "$$P(\\theta) = 0.5 \\cdot N(\\theta; [-3, 0], I) + 0.5 \\cdot N(\\theta; [3, 0], I)$$\n",
    "\n",
    "This is difficult for standard MCMC because the modes are separated, requiring large jumps to transition between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_posterior_bimodal(theta):\n",
    "    \"\"\"Log-posterior for bimodal Gaussian mixture.\"\"\"\n",
    "    # Two modes at [-3, 0] and [3, 0]\n",
    "    log_p1 = stats.multivariate_normal.logpdf(theta, mean=[-3, 0])\n",
    "    log_p2 = stats.multivariate_normal.logpdf(theta, mean=[3, 0])\n",
    "    \n",
    "    # Log of mixture: log(0.5 * exp(log_p1) + 0.5 * exp(log_p2))\n",
    "    max_log = max(log_p1, log_p2)\n",
    "    return max_log + np.log(0.5 * np.exp(log_p1 - max_log) + 0.5 * np.exp(log_p2 - max_log))\n",
    "\n",
    "# Standard MH (for comparison)\n",
    "print(\"Standard Metropolis-Hastings:\")\n",
    "samples_mh, accepted_mh, info_mh = diffusion_assisted_mcmc(\n",
    "    log_posterior_bimodal,\n",
    "    np.array([0.0, 0.0]),\n",
    "    n_samples=3000,\n",
    "    p_diff=0.0,  # No diffusion\n",
    "    sigma_mh=0.5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Diffusion-assisted MH (seed with samples near both modes)\n",
    "print(\"\\nDiffusion-Assisted MCMC:\")\n",
    "seed_samples = np.array([[-3, 0], [3, 0], [-3, 0], [3, 0]])  # Seed both modes\n",
    "samples_diff, accepted_diff, info_diff = diffusion_assisted_mcmc(\n",
    "    log_posterior_bimodal,\n",
    "    np.array([0.0, 0.0]),\n",
    "    n_samples=3000,\n",
    "    p_diff=0.5,\n",
    "    sigma_mh=0.5,\n",
    "    seed_samples=seed_samples,\n",
    "    retrain_interval=300,\n",
    "    random_state=43,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "burn_in = 500\n",
    "\n",
    "# Standard MH\n",
    "axes[0, 0].plot(samples_mh[:, 0], alpha=0.7, color=\"#332288\", linewidth=0.5)\n",
    "axes[0, 0].axvline(burn_in, color=\"#CC6677\", linestyle=\"--\", alpha=0.7)\n",
    "axes[0, 0].set_xlabel(\"Iteration\")\n",
    "axes[0, 0].set_ylabel(r\"$\\theta_1$\")\n",
    "axes[0, 0].set_title(\"Standard MH: Trace Plot\")\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "samples_mh_burned = samples_mh[burn_in:]\n",
    "axes[0, 1].scatter(samples_mh_burned[:, 0], samples_mh_burned[:, 1], \n",
    "                   alpha=0.4, s=2, color=\"#332288\")\n",
    "axes[0, 1].set_xlabel(r\"$\\theta_1$\")\n",
    "axes[0, 1].set_ylabel(r\"$\\theta_2$\")\n",
    "axes[0, 1].set_title(\"Standard MH: Samples\")\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "axes[0, 1].axis(\"equal\")\n",
    "\n",
    "# Count mode visits\n",
    "mh_left = np.sum(samples_mh_burned[:, 0] < 0)\n",
    "mh_right = np.sum(samples_mh_burned[:, 0] > 0)\n",
    "axes[0, 1].text(0.02, 0.98, f\"Left mode: {mh_left}\\nRight mode: {mh_right}\",\n",
    "                transform=axes[0, 1].transAxes, verticalalignment=\"top\",\n",
    "                bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "# Diffusion-assisted\n",
    "axes[1, 0].plot(samples_diff[:, 0], alpha=0.7, color=\"#117733\", linewidth=0.5)\n",
    "axes[1, 0].axvline(burn_in, color=\"#CC6677\", linestyle=\"--\", alpha=0.7)\n",
    "axes[1, 0].set_xlabel(\"Iteration\")\n",
    "axes[1, 0].set_ylabel(r\"$\\theta_1$\")\n",
    "axes[1, 0].set_title(\"Diffusion-Assisted: Trace Plot\")\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "samples_diff_burned = samples_diff[burn_in:]\n",
    "axes[1, 1].scatter(samples_diff_burned[:, 0], samples_diff_burned[:, 1], \n",
    "                   alpha=0.4, s=2, color=\"#117733\")\n",
    "axes[1, 1].set_xlabel(r\"$\\theta_1$\")\n",
    "axes[1, 1].set_ylabel(r\"$\\theta_2$\")\n",
    "axes[1, 1].set_title(\"Diffusion-Assisted: Samples\")\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "axes[1, 1].axis(\"equal\")\n",
    "\n",
    "# Count mode visits\n",
    "diff_left = np.sum(samples_diff_burned[:, 0] < 0)\n",
    "diff_right = np.sum(samples_diff_burned[:, 0] > 0)\n",
    "axes[1, 1].text(0.02, 0.98, f\"Left mode: {diff_left}\\nRight mode: {diff_right}\",\n",
    "                transform=axes[1, 1].transAxes, verticalalignment=\"top\",\n",
    "                bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Mode Exploration Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Standard MH:\")\n",
    "print(f\"  Left mode:  {mh_left:4d} samples ({mh_left/len(samples_mh_burned)*100:.1f}%)\")\n",
    "print(f\"  Right mode: {mh_right:4d} samples ({mh_right/len(samples_mh_burned)*100:.1f}%)\")\n",
    "print(f\"  Balance: {min(mh_left, mh_right)/max(mh_left, mh_right):.2f}\")\n",
    "print(f\"\\nDiffusion-Assisted:\")\n",
    "print(f\"  Left mode:  {diff_left:4d} samples ({diff_left/len(samples_diff_burned)*100:.1f}%)\")\n",
    "print(f\"  Right mode: {diff_right:4d} samples ({diff_right/len(samples_diff_burned)*100:.1f}%)\")\n",
    "print(f\"  Balance: {min(diff_left, diff_right)/max(diff_left, diff_right):.2f}\")\n",
    "print(\"\\nNote: Balance closer to 1.0 indicates better exploration of both modes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Himmelblau Function (4 Modes)\n",
    "\n",
    "The Himmelblau function is a famous optimization test function with 4 minima of equal depth:\n",
    "\n",
    "$$f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$$\n",
    "\n",
    "The four minima (modes for our posterior) are located at:\n",
    "- $(3.0, 2.0)$\n",
    "- $(-2.8, 3.1)$\n",
    "- $(-3.8, -3.3)$\n",
    "- $(3.6, -1.8)$\n",
    "\n",
    "For MCMC, we define the log-posterior as $\\log P(\\theta) = -f(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_posterior_himmelblau(theta):\n",
    "    \"\"\"Log-posterior based on Himmelblau function (4 modes).\"\"\"\n",
    "    x, y = theta\n",
    "    val = (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n",
    "    return -val  # Negative for log-posterior (higher is better)\n",
    "\n",
    "# Seed with points near all four modes\n",
    "himmelblau_modes = np.array([\n",
    "    [3.0, 2.0],\n",
    "    [-2.8, 3.1],\n",
    "    [-3.8, -3.3],\n",
    "    [3.6, -1.8]\n",
    "])\n",
    "\n",
    "# Add some noise to seed samples\n",
    "seed_samples = himmelblau_modes + np.random.randn(*himmelblau_modes.shape) * 0.5\n",
    "\n",
    "print(\"Sampling from Himmelblau function (4 modes)...\")\n",
    "samples_himmel, accepted_himmel, info_himmel = diffusion_assisted_mcmc(\n",
    "    log_posterior_himmelblau,\n",
    "    himmelblau_modes[0],  # Start at first mode\n",
    "    n_samples=5000,\n",
    "    seed_samples=seed_samples,\n",
    "    p_diff=0.8,  # High probability of diffusion proposals\n",
    "    sigma_mh=0.15,\n",
    "    retrain_interval=500,\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Himmelblau sampling\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "burn_in = 1000\n",
    "samples_himmel_burned = samples_himmel[burn_in:]\n",
    "\n",
    "# Create contour plot of Himmelblau function\n",
    "x_range = np.linspace(-5, 5, 200)\n",
    "y_range = np.linspace(-5, 5, 200)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = -np.array([[log_posterior_himmelblau([x, y]) for x in x_range] for y in y_range])\n",
    "\n",
    "# Plot 1: Samples with contours\n",
    "contour = axes[0].contourf(X, Y, Z, levels=20, cmap=\"viridis\", alpha=0.6)\n",
    "axes[0].scatter(samples_himmel_burned[:, 0], samples_himmel_burned[:, 1],\n",
    "                c=\"#CC6677\", s=1, alpha=0.5, label=\"Samples\")\n",
    "axes[0].scatter(himmelblau_modes[:, 0], himmelblau_modes[:, 1],\n",
    "                c=\"white\", s=100, marker=\"*\", edgecolors=\"black\", linewidths=2,\n",
    "                label=\"True modes\", zorder=5)\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"y\")\n",
    "axes[0].set_title(\"Samples on Himmelblau Function\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "plt.colorbar(contour, ax=axes[0], label=\"f(x, y)\")\n",
    "\n",
    "# Plot 2: Trace plot showing mode jumps\n",
    "axes[1].plot(samples_himmel[:, 0], alpha=0.7, color=\"#117733\", linewidth=0.5)\n",
    "axes[1].axvline(burn_in, color=\"#CC6677\", linestyle=\"--\", alpha=0.7, label=\"Burn-in\")\n",
    "for i, mode in enumerate(himmelblau_modes):\n",
    "    axes[1].axhline(mode[0], color=\"#332288\", linestyle=\":\", alpha=0.5)\n",
    "axes[1].set_xlabel(\"Iteration\")\n",
    "axes[1].set_ylabel(\"x\")\n",
    "axes[1].set_title(\"Trace Plot (showing mode jumps)\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count visits to each mode\n",
    "mode_counts = []\n",
    "mode_regions = [\n",
    "    (2, 5, 1, 3),      # Mode 1: x in [2,5], y in [1,3]\n",
    "    (-4, -2, 2, 4),    # Mode 2: x in [-4,-2], y in [2,4]\n",
    "    (-5, -3, -4, -2),  # Mode 3: x in [-5,-3], y in [-4,-2]\n",
    "    (2, 5, -3, 0),     # Mode 4: x in [2,5], y in [-3,0]\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Mode Visit Statistics:\")\n",
    "print(\"=\"*60)\n",
    "for i, (x_min, x_max, y_min, y_max) in enumerate(mode_regions, 1):\n",
    "    mask = (samples_himmel_burned[:, 0] >= x_min) & (samples_himmel_burned[:, 0] <= x_max) & \\\n",
    "           (samples_himmel_burned[:, 1] >= y_min) & (samples_himmel_burned[:, 1] <= y_max)\n",
    "    count = np.sum(mask)\n",
    "    percentage = count / len(samples_himmel_burned) * 100\n",
    "    print(f\"Mode {i} at {himmelblau_modes[i-1]}: {count:4d} samples ({percentage:5.1f}%)\")\n",
    "    mode_counts.append(count)\n",
    "\n",
    "modes_visited = sum(c > 50 for c in mode_counts)\n",
    "print(f\"\\nNumber of modes with >50 samples: {modes_visited}/4\")\n",
    "print(f\"\\nThis demonstrates the algorithm's ability to explore multiple modes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Multi-modal sampling**: Diffusion-assisted MCMC can effectively jump between distant modes, which standard MCMC struggles with.\n",
    "\n",
    "2. **Adaptive learning**: The diffusion model improves as sampling progresses, learning the shape of the posterior.\n",
    "\n",
    "3. **No gradients needed**: Unlike some advanced methods (HMC, MALA), this works with black-box posteriors.\n",
    "\n",
    "4. **Asymptotic exactness**: Despite using an approximate proposal, the acceptance step ensures convergence to the true posterior.\n",
    "\n",
    "5. **Hyperparameter tuning**: Key parameters are:\n",
    "   - `p_diff`: Balance between local and global proposals (0.3-0.8 typically works well)\n",
    "   - `sigma_mh`: Step size for local proposals\n",
    "   - `retrain_interval`: How often to update the diffusion model (trade-off with compute time)\n",
    "   - `seed_samples`: For multi-modal problems, seeding near all modes helps\n",
    "\n",
    "## When to Use This Algorithm\n",
    "\n",
    "**Good for:**\n",
    "- Multi-modal posteriors\n",
    "- Posteriors with well-separated regions of high probability\n",
    "- Black-box likelihoods (no gradient information)\n",
    "- When you want a learned approximation of the posterior for fast sampling\n",
    "\n",
    "**Maybe not needed for:**\n",
    "- Simple unimodal Gaussians (standard MCMC is fine)\n",
    "- When you have gradient information (consider HMC/NUTS instead)\n",
    "- Very high dimensions (diffusion model training becomes expensive)\n",
    "\n",
    "## References\n",
    "\n",
    "- Hunt-Smith et al. (2023), \"Accelerating Markov Chain Monte Carlo sampling with diffusion models\", arXiv:2309.01454v1\n",
    "- Implementation available at: https://github.com/NickHunt-Smith/MCMC-diffusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
