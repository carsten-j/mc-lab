\begin{frame}
	\vspace{2cm}
	\begin{center}
		{\Huge\textbf{\textcolor{copenhagenred}{Importance Sampling}}}
		\vspace{1cm}

		\rule{4cm}{3pt}
		\vspace{2cm}
	\end{center}
\end{frame}

% % Slide 1: What, Why, and How
\begin{frame}{What, Why, and How}
	\begin{columns}
		\begin{column}{0.48\textwidth}
			\textbf{What?}
			\begin{itemize}
				\item Monte Carlo technique for estimating $\mathbb{E}_\pi[\phi(X)]$
				\item Sample from proposal $q$ instead of target $\pi$
				\item Reweight samples to correct bias
			\end{itemize}

			\vspace{0.3cm}
			\textbf{Why?}
			\begin{itemize}
				\item Target $\pi$ difficult to sample from
				\item Focus sampling in important regions
				\item Works with unnormalized distributions
			\end{itemize}
		\end{column}

		\begin{column}{0.48\textwidth}
			\textbf{How?} The key identity:
			\begin{align*}
				I = \mathbb{E}_\pi[\phi(X)] & = \int \phi(x)\pi(x)dx              \\
				                        & = \int \phi(x)\frac{\pi(x)}{q(x)}q(x)dx \\
				                        & = \mathbb{E}_q[\phi(X)w(X)]
			\end{align*}

			\begin{block}{Algorithm}
				\begin{enumerate}
					\item Sample $X_1, \ldots, X_n \sim q$
					\item Compute $w(X_i) = \pi(X_i)/q(X_i)$
					\item Estimate: $\hat{I}_n = \frac{1}{n}\sum_{i=1}^n \phi(X_i)w(X_i)$
				\end{enumerate}
			\end{block}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Requirements, Properties and Unnormalized case}
	\begin{columns}
		\begin{column}{0.55\textwidth}
			\textbf{Requirements:}
			\begin{itemize}
				\item $q(x) > 0$ whenever $\pi(x) > 0$
				% \item $\mathbb{E}_q[|\phi(X)w(X)|] < \infty$
			\end{itemize}
			\vspace{0.3cm}
			\textbf{Properties of IS Estimator:}
			\begin{itemize}
				\item \textbf{Unbiased}: $\mathbb{E}_q[\hat{I}_n] = \mathbb{E}_\pi[\phi(X)]$
				\item \textbf{Consistent}: $\lim\limits_{n \to \infty} \hat{I}_n = I$ (LLN)
				\item \textbf{Variance}: $\text{Var}_q[\hat{I}_n] = \frac{1}{n}\text{Var}_q[\phi(X)w(X)]$ (CLT)
			\end{itemize}
		\end{column}

		\begin{column}{0.45\textwidth}
			\textbf{Unnormalized Distributions:}\\
			When $\pi(x) = \tilde{\pi}(x)/Z_{\pi}$ and $q(x) = \tilde{q}(x)/Z_{q}$:

			\begin{block}{Self-Normalized IS}
				\begin{itemize}
					\item Weights: $\tilde{w}(x) = \tilde{\pi}(x)/\tilde{q}(x)$
					\item Estimator: $$\hat{I}_{NIS} = \frac{\sum_{i=1}^n \phi(X_i)\tilde{w}(X_i)}{\sum_{i=1}^n \tilde{w}(X_i)}$$
					\item Biased but consistent
				\end{itemize}
			\end{block}
		\end{column}
	\end{columns}
	\tiny
	$$\mathbb{E}_q[\hat{I}_n] = \mathbb{E}_q\left[\frac{1}{N} \sum_{i=1}^{N} \phi(x_i) w(x_i)\right] = \frac{1}{N} N  \mathbb{E}_q[\phi(X) w(X)] = \mathbb{E}_q[\phi(X) w(X)] = \int \phi(x)  w(x)  q(x) \, dx = \int \phi(x)  \frac{p(x)}{q(x)}  q(x) \, dx = \int \phi(x) \, p(x) \, dx = \mathbb{E}_p[\phi(X)]$$
\end{frame}

\begin{frame}{Importance sampling diagnostics}
	In extreme settings, one of the $w_i$ may be vastly larger than all the others and
	then we have effectively only got one observation.
	
	\vspace{0.3cm}
	\begin{itemize}
		\item High Variance from Large Weights. If $\pi(x)\gg q(x)$ in some regions, you get huge weights $w(x) = \frac{\pi(x)}{q(x)}$. Even rare samples from these regions cause massive variance.
	\end{itemize}

	On the other end of the spectrum all the weights could be very small if $q$ places too much 
	mass in regions where $\pi$ is negligible, i.e. $q(x) \gg \pi(x)$.

	\vspace{0.3cm}
	\begin{itemize}
		\item Most weights $ \approx 0$ (negligible contribution)
	\end{itemize}

	$$\text{Var}_q[\hat{I}_n] = \frac{1}{N} \text{Var}_q[\phi(X) w(X)] = \frac{1}{N} \Big(\mathbb{E}_q[\phi^2(X) w^2(X)] - (\mathbb{E}_p[\phi(X) w(X)])^2\Big)$$
\end{frame}

\begin{frame}{Effective Sample Size (ESS)}
	\begin{block}{Definition}
		$$\text{ESS} = \frac{(\sum_{i=1}^n w_i)^2}{\sum_{i=1}^n w_i^2}$$
	\end{block}

	\textbf{Interpretation:}
	\begin{itemize}
		\item Number of "equivalent" samples from $\pi$
		\item Range: $1 \leq \text{ESS} \leq n$
		\item ESS $= n$ when all weights equal, ESS $= 1$ when one weight dominates
	\end{itemize}

	\textbf{Note:}
	If all weights are close to 0, ESS will still be close to $n$ even though weights are not informative.
	One way to check for this $\sum_{i=1}^N w_i \approx N$ (since $\mathbb{E}_q[w(X)] = 1$).
\end{frame}

\begin{frame}{What Makes a Good Proposal Distribution?}
	\textbf{Ideal}: $q(x) \propto |\phi(x)| \pi(x)$ (as it minimizes variance $V_q(\phi(X) w(X))$)

	\vspace{0.3cm}
	\textbf{Practical guidelines}:
	\begin{itemize}
		\item Heavy tails: $q$ should have heavier tails than $\pi$ (importance region coverage)
		\item Support: $q(x) > 0$ wherever $\phi(x)\pi(x) \neq 0$
		\item Similar shape: $q$ should roughly match the shape of $\pi$, especially where $|\phi(x)|$ is large
	\end{itemize}
\end{frame}

\begin{frame}{Dimensional Scaling}
	\textbf{Curse of Dimensionality:}
	\begin{block}{Gaussian Example}
		For $\pi = \mathcal{N}(0,I_d)$, $q = \mathcal{N}(0,\sigma^2 I_d)$:
		$$\text{Var}_q[w(X)] = \left(\frac{\sigma^4}{2\sigma^2-1}\right)^{d/2} - 1$$
	\end{block}

	\textbf{Numerical Example:}
	
	\vspace{0.3cm}
	If we set
	$d = 100$, $\sigma = 1.2$, then $\text{Var}_q[w(X)]$ is approximately $1.8 \times 10^4$.
\end{frame}
