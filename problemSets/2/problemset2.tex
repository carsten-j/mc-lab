\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{geometry}
\usepackage{enumitem}

\geometry{margin=1in}

\title{Topics in Statistics: Markov Chain Monte Carlo\\Problem Set 2 - Solutions}
\author{}
\date{}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}

\begin{document}

\maketitle

\section{Exercise 1}

\subsection{Part 1: Full conditional distributions}

Given: $\pi(x, y) \propto \exp\left(-\frac{1}{2}(x - 1)^2(y - 2)^2\right)$

For the conditional distribution $\pi(x|y)$:
\begin{align}
\pi(x|y) &\propto \exp\left(-\frac{1}{2}(x - 1)^2(y - 2)^2\right)
\end{align}

Since we condition on $y$, the term $(y - 2)^2$ acts as a constant. Let $c = (y - 2)^2$:
\begin{align}
\pi(x|y) &\propto \exp\left(-\frac{c}{2}(x - 1)^2\right)
\end{align}

This is a normal distribution: $X|Y=y \sim \mathcal{N}\left(1, \frac{1}{(y-2)^2}\right)$ when $y \neq 2$.

Similarly, for $\pi(y|x)$:
\begin{align}
\pi(y|x) &\propto \exp\left(-\frac{1}{2}(x - 1)^2(y - 2)^2\right)
\end{align}

Let $d = (x - 1)^2$:
\begin{align}
\pi(y|x) &\propto \exp\left(-\frac{d}{2}(y - 2)^2\right)
\end{align}

This gives us: $Y|X=x \sim \mathcal{N}\left(2, \frac{1}{(x-1)^2}\right)$ when $x \neq 1$.

\subsection{Part 2: Does the Gibbs sampler make sense?}

No, this Gibbs sampler does not make sense. The problem arises because:

\begin{enumerate}
\item When $x = 1$, the conditional variance of $Y$ becomes infinite (since $\frac{1}{(x-1)^2} = \frac{1}{0}$)
\item When $y = 2$, the conditional variance of $X$ becomes infinite (since $\frac{1}{(y-2)^2} = \frac{1}{0}$)
\item If the chain ever reaches the point $(1, 2)$, both conditional distributions become degenerate
\end{enumerate}

The joint distribution has a ridge along the lines $x = 1$ and $y = 2$, making the conditional distributions ill-defined at these points.

\section{Exercise 2}

\subsection{Part 1: Likelihood function}

We observe $Z_i = X_i + Y_i$ where $X_i \sim \text{Binomial}(m_i, \theta_1)$ and $Y_i \sim \text{Binomial}(n_i, \theta_2)$.

For each observation $z_i$, we need to sum over all possible ways to achieve $Z_i = z_i$:

\begin{align}
P(Z_i = z_i | \theta_1, \theta_2) &= \sum_{x_i=\max(0, z_i-n_i)}^{\min(m_i, z_i)} P(X_i = x_i | \theta_1) P(Y_i = z_i - x_i | \theta_2)\\
&= \sum_{x_i=\max(0, z_i-n_i)}^{\min(m_i, z_i)} \binom{m_i}{x_i} \theta_1^{x_i} (1-\theta_1)^{m_i-x_i} \binom{n_i}{z_i-x_i} \theta_2^{z_i-x_i} (1-\theta_2)^{n_i-(z_i-x_i)}
\end{align}

The likelihood function is:
\begin{align}
&p(z_1, \ldots, z_T | \theta_1, \theta_2) \\
&= \prod_{i=1}^T \sum_{x_i=\max(0, z_i-n_i)}^{\min(m_i, z_i)} \binom{m_i}{x_i} \binom{n_i}{z_i-x_i} \theta_1^{x_i} (1-\theta_1)^{m_i-x_i} \theta_2^{z_i-x_i} (1-\theta_2)^{n_i-(z_i-x_i)}
\end{align}

\subsection{Part 2: Gibbs sampler with auxiliary variables}

Following the hint, we introduce auxiliary variables $X_1, \ldots, X_T$ representing the unobserved $X_i$ values.

The augmented posterior is:
\begin{align}
&p(\theta_1, \theta_2, x_1, \ldots, x_T | z_1, \ldots, z_T) \\
&\propto \prod_{i=1}^T \binom{m_i}{x_i} \binom{n_i}{z_i-x_i} \theta_1^{x_i} (1-\theta_1)^{m_i-x_i} \theta_2^{z_i-x_i} (1-\theta_2)^{n_i-(z_i-x_i)}
\end{align}

\textbf{Full conditional distributions:}

\begin{enumerate}
\item For $\theta_1$:
\begin{align}
\theta_1 | \cdots \sim \text{Beta}\left(\sum_{i=1}^T x_i + 1, \sum_{i=1}^T (m_i - x_i) + 1\right)
\end{align}

\item For $\theta_2$:
\begin{align}
\theta_2 | \cdots \sim \text{Beta}\left(\sum_{i=1}^T (z_i - x_i) + 1, \sum_{i=1}^T (n_i - z_i + x_i) + 1\right)
\end{align}

\item For each $x_i$:
\begin{align}
P(X_i = x_i | \cdots) &\propto \binom{m_i}{x_i} \binom{n_i}{z_i-x_i} \theta_1^{x_i} (1-\theta_1)^{m_i-x_i} \theta_2^{z_i-x_i} (1-\theta_2)^{n_i-(z_i-x_i)}
\end{align}
where $x_i \in \{\max(0, z_i - n_i), \ldots, \min(m_i, z_i)\}$.
\end{enumerate}

\textbf{Gibbs sampler algorithm:}
\begin{enumerate}
\item Sample $\theta_1$ from its Beta conditional
\item Sample $\theta_2$ from its Beta conditional  
\item For each $i = 1, \ldots, T$, sample $x_i$ from its discrete conditional distribution
\item Repeat steps 1-3
\end{enumerate}

\section{Exercise 4}

\subsection{Part 1: Prove Cauchy-Schwarz inequality}

We need to prove that for any two real-valued random variables $Y$ and $Z$:
\begin{align}
|E[YZ]|^2 \leq E[Y^2]E[Z^2]
\end{align}

\textbf{Proof using the hint:}

Consider $(Y - \alpha Z)^2 \geq 0$ for any $\alpha \in \mathbb{R}$.

Expanding this inequality:
\begin{align}
E[(Y - \alpha Z)^2] &\geq 0\\
E[Y^2 - 2\alpha YZ + \alpha^2 Z^2] &\geq 0\\
E[Y^2] - 2\alpha E[YZ] + \alpha^2 E[Z^2] &\geq 0
\end{align}

This is a quadratic function in $\alpha$ that is always non-negative. For a quadratic $a\alpha^2 + b\alpha + c \geq 0$ to hold for all $\alpha$, we need the discriminant to be non-positive:
\begin{align}
b^2 - 4ac \leq 0
\end{align}

In our case:
\begin{itemize}
\item $a = E[Z^2]$
\item $b = -2E[YZ]$ 
\item $c = E[Y^2]$
\end{itemize}

So we need:
\begin{align}
(-2E[YZ])^2 - 4E[Z^2]E[Y^2] &\leq 0\\
4(E[YZ])^2 - 4E[Z^2]E[Y^2] &\leq 0\\
(E[YZ])^2 &\leq E[Z^2]E[Y^2]
\end{align}

Taking square roots: $|E[YZ]|^2 \leq E[Y^2]E[Z^2]$ $\square$

\subsection{Part 2: Show that when marginal distributions are identical, $Cov(Y,Z) \leq Var(Y)$}

When $Y$ and $Z$ have identical marginal distributions, we have:
\begin{itemize}
\item $E[Y] = E[Z]$ (let's call this $\mu$)
\item $E[Y^2] = E[Z^2]$ 
\item $\text{Var}(Y) = \text{Var}(Z)$ (let's call this $\sigma^2$)
\end{itemize}

By definition:
\begin{align}
\text{Cov}(Y,Z) = E[YZ] - E[Y]E[Z] = E[YZ] - \mu^2
\end{align}

From Cauchy-Schwarz inequality:
\begin{align}
|E[YZ]|^2 \leq E[Y^2]E[Z^2]
\end{align}

Since marginal distributions are identical:
\begin{align}
|E[YZ]|^2 \leq (E[Y^2])^2
\end{align}

Therefore: $|E[YZ]| \leq E[Y^2]$

This gives us: $-E[Y^2] \leq E[YZ] \leq E[Y^2]$

For the covariance:
\begin{align}
\text{Cov}(Y,Z) = E[YZ] - \mu^2 \leq E[Y^2] - \mu^2 = \text{Var}(Y)
\end{align}

Therefore: $\text{Cov}(Y,Z) \leq \text{Var}(Y)$ $\square$

\subsection{Part 3: Thinning inequality}

We need to show that for a stationary Markov chain $\{X^{(t)}\}_{t \geq 0}$ with $T \geq m > 1$ and $T/m \in \mathbb{N}$:

\begin{align}
\text{Var}\left[\frac{1}{T}\sum_{t=0}^{T-1} X^{(t)}\right] \leq \text{Var}\left[\frac{1}{T/m}\sum_{t=0}^{T/m-1} Y^{(t)}\right]
\end{align}

where $Y^{(t)} := X^{(m \cdot t)}$ is the thinned chain.

\textbf{Proof using the hint:}

Following the hint, we write:
\begin{align}
\sum_{t=0}^{T-1} X^{(t)} = \sum_{j=0}^{m-1} \sum_{s=0}^{T/m-1} X^{(s \cdot m + j)}
\end{align}

Let $S_j = \sum_{s=0}^{T/m-1} X^{(s \cdot m + j)}$ for $j = 0, 1, \ldots, m-1$.

Then:
\begin{align}
\sum_{t=0}^{T-1} X^{(t)} = \sum_{j=0}^{m-1} S_j
\end{align}

Note that $S_0 = \sum_{s=0}^{T/m-1} X^{(s \cdot m)} = \sum_{t=0}^{T/m-1} Y^{(t)}$ (the thinned chain sum).

For the variance:
\begin{align}
\text{Var}\left[\sum_{j=0}^{m-1} S_j\right] = \sum_{j=0}^{m-1} \text{Var}[S_j] + 2\sum_{j=0}^{m-1}\sum_{k=j+1}^{m-1} \text{Cov}(S_j, S_k)
\end{align}

Since the chain is stationary, all $S_j$ have the same marginal distribution, so:
\begin{align}
\text{Var}[S_j] = \text{Var}[S_0] \text{ for all } j
\end{align}

By Part 2, since $S_j$ and $S_k$ have identical marginal distributions:
\begin{align}
\text{Cov}(S_j, S_k) \leq \text{Var}(S_j) = \text{Var}(S_0)
\end{align}

Therefore:
\begin{align}
\text{Var}\left[\sum_{j=0}^{m-1} S_j\right] &\leq m \cdot \text{Var}[S_0] + 2\binom{m}{2} \text{Var}[S_0]\\
&= m \cdot \text{Var}[S_0] + m(m-1) \text{Var}[S_0]\\
&= m^2 \text{Var}[S_0]
\end{align}

Dividing by $T^2$:
\begin{align}
\text{Var}\left[\frac{1}{T}\sum_{t=0}^{T-1} X^{(t)}\right] &\leq \frac{m^2}{T^2} \text{Var}[S_0]\\
&= \frac{1}{(T/m)^2} \text{Var}[S_0]\\
&= \text{Var}\left[\frac{1}{T/m}\sum_{t=0}^{T/m-1} Y^{(t)}\right]
\end{align}

\textbf{What this tells us about thinning:}

This result shows that \textbf{thinning increases variance}. The original chain $\{X^{(t)}\}$ produces estimates with lower variance than the thinned chain $\{Y^{(t)}\}$. This happens because:

\begin{enumerate}
\item Using all samples is better than discarding information
\item Even though thinned samples are less correlated, we lose more information than we gain
\item Modern consensus aligns with this - thinning is generally not recommended unless storage is a constraint
\end{enumerate}

\section{Exercise 6}

\subsection{Part 1: MWG reduces to systematic scan Gibbs sampler}

When $q(x'_1|x_1, x_2) = \pi_{X_1|X_2}(x'_1|x_2)$, the acceptance probability becomes:

\begin{align}
\alpha(X_1|X_1^{(t-1)}, X_2^{(t-1)}) &= \min\left\{1, \frac{\pi(X_1, X_2^{(t-1)})q(X_1^{(t-1)}|X_1, X_2^{(t-1)})}{\pi(X_1^{(t-1)}, X_2^{(t-1)})q(X_1|X_1^{(t-1)}, X_2^{(t-1)})}\right\}
\end{align}

Substituting $q(x'_1|x_1, x_2) = \pi_{X_1|X_2}(x'_1|x_2)$:

\begin{align}
\alpha(X_1|X_1^{(t-1)}, X_2^{(t-1)}) &= \min\left\{1, \frac{\pi(X_1, X_2^{(t-1)})\pi_{X_1|X_2}(X_1^{(t-1)}|X_2^{(t-1)})}{\pi(X_1^{(t-1)}, X_2^{(t-1)})\pi_{X_1|X_2}(X_1|X_2^{(t-1)})}\right\}
\end{align}

Using $\pi(x_1, x_2) = \pi_{X_1|X_2}(x_1|x_2)\pi_{X_2}(x_2)$:

\begin{align}
&= \min\left\{1, \frac{\pi_{X_1|X_2}(X_1|X_2^{(t-1)})\pi_{X_2}(X_2^{(t-1)})\pi_{X_1|X_2}(X_1^{(t-1)}|X_2^{(t-1)})}{\pi_{X_1|X_2}(X_1^{(t-1)}|X_2^{(t-1)})\pi_{X_2}(X_2^{(t-1)})\pi_{X_1|X_2}(X_1|X_2^{(t-1)})}\right\}\\
&= \min\{1, 1\} = 1
\end{align}

Since the acceptance probability is always 1, every proposed move is accepted. This means:
\begin{itemize}
\item $X_1^{(t)}$ is always sampled from $\pi_{X_1|X_2}(\cdot|X_2^{(t-1)})$
\item $X_2^{(t)}$ is sampled from $\pi_{X_2|X_1}(\cdot|X_1^{(t)})$
\end{itemize}

This is exactly the systematic scan Gibbs sampler! $\square$

\subsection{Part 2: Transition kernel and invariant distribution}

The transition kernel $K((x_1, x_2), (x'_1, x'_2))$ can be written as:

\begin{align}
K((x_1, x_2), (x'_1, x'_2)) = K_{MH}((x_1, x_2), (x'_1, x_2)) \cdot \pi_{X_2|X_1}(x'_2|x'_1)
\end{align}

where $K_{MH}$ is the standard Metropolis-Hastings kernel:
\begin{align}
K_{MH}((x_1, x_2), (x'_1, x_2)) &= q(x'_1|x_1, x_2)\alpha(x'_1|x_1, x_2)\\
&\quad + \delta_{x_1}(x'_1)\left(1 - \int_{X_1} q(u_1|x_1, x_2)\alpha(u_1|x_1, x_2)du_1\right)
\end{align}

\textbf{Proof that $\pi$ is invariant:}

The MWG algorithm consists of two sequential steps:
\begin{enumerate}
\item Metropolis-Hastings update for $X_1$ (keeping $X_2$ fixed)  
\item Gibbs update for $X_2$ (given the new $X_1$)
\end{enumerate}

\textbf{Step 1 preserves $\pi$:} 
For any fixed value $x_2$, the MH algorithm with proposal $q(\cdot|x_1, x_2)$ and the given acceptance probability is designed to have stationary distribution $\pi_{X_1|X_2}(\cdot|x_2)$. 

Since we start with $X_1^{(t-1)}|X_2^{(t-1)} \sim \pi_{X_1|X_2}(\cdot|X_2^{(t-1)})$, after the MH step we still have $X_1^*|X_2^{(t-1)} \sim \pi_{X_1|X_2}(\cdot|X_2^{(t-1)})$. Therefore $(X_1^*, X_2^{(t-1)}) \sim \pi$.

\textbf{Step 2 preserves $\pi$:} 
Starting with $(X_1^*, X_2^{(t-1)}) \sim \pi$, we sample $X_2^{(t)} \sim \pi_{X_2|X_1}(\cdot|X_1^*)$. By the definition of conditional distributions, this gives $(X_1^*, X_2^{(t)}) \sim \pi$.

Therefore, starting with a sample from $\pi$ and applying one iteration of MWG gives another sample from $\pi$, proving that $\pi$ is the invariant distribution. $\square$

\section{Exercise 7}

\subsection{Question 1: Show T is $\pi$-reversible}

We need to show that $\pi(x)T(x,y) = \pi(y)T(y,x)$ for all $x,y \in X$.

Given:
\begin{itemize}
\item $\alpha(x,y) = \frac{\gamma(x,y)}{\pi(x)q(x,y)}$
\item $\gamma(x,y) = \gamma(y,x)$ (symmetry condition)
\item $0 \leq \alpha(x,y) \leq 1$
\end{itemize}

The transition kernel is:
\begin{align}
T(x,y) = \alpha(x,y)q(x,y) + \left(1 - \sum_{z \in X} \alpha(x,z)q(x,z)\right)\delta_x(y)
\end{align}

\textbf{Case 1: $x \neq y$}

In this case, $\delta_x(y) = 0$, so:
\begin{align}
T(x,y) &= \alpha(x,y)q(x,y) = \frac{\gamma(x,y)}{\pi(x)q(x,y)} \cdot q(x,y) = \frac{\gamma(x,y)}{\pi(x)}
\end{align}

Similarly:
\begin{align}
T(y,x) &= \alpha(y,x)q(y,x) = \frac{\gamma(y,x)}{\pi(y)q(y,x)} \cdot q(y,x) = \frac{\gamma(y,x)}{\pi(y)}
\end{align}

Now checking detailed balance:
\begin{align}
\pi(x)T(x,y) &= \pi(x) \cdot \frac{\gamma(x,y)}{\pi(x)} = \gamma(x,y)\\
\pi(y)T(y,x) &= \pi(y) \cdot \frac{\gamma(y,x)}{\pi(y)} = \gamma(y,x)
\end{align}

Since $\gamma(x,y) = \gamma(y,x)$, we have:
\begin{align}
\pi(x)T(x,y) = \pi(y)T(y,x)
\end{align}

\textbf{Case 2: $x = y$}

For $x = y$, the detailed balance condition $\pi(x)T(x,x) = \pi(x)T(x,x)$ is trivially satisfied.

Therefore, T is $\pi$-reversible. $\square$

\subsection{Question 2: Verify Metropolis-Hastings and find Baker acceptance probability}

\subsubsection{Part (a): Metropolis-Hastings}

For Metropolis-Hastings, we have:
\begin{align}
\gamma(x,y) = \min\{\pi(x)q(x,y), \pi(y)q(y,x)\}
\end{align}

The acceptance probability is:
\begin{align}
\alpha(x,y) = \frac{\gamma(x,y)}{\pi(x)q(x,y)} = \frac{\min\{\pi(x)q(x,y), \pi(y)q(y,x)\}}{\pi(x)q(x,y)}
\end{align}

\textbf{Case 1:} If $\pi(x)q(x,y) \leq \pi(y)q(y,x)$, then:
\begin{align}
\alpha(x,y) = \frac{\pi(x)q(x,y)}{\pi(x)q(x,y)} = 1
\end{align}

\textbf{Case 2:} If $\pi(x)q(x,y) > \pi(y)q(y,x)$, then:
\begin{align}
\alpha(x,y) = \frac{\pi(y)q(y,x)}{\pi(x)q(x,y)}
\end{align}

Combining both cases:
\begin{align}
\alpha(x,y) = \min\left\{1, \frac{\pi(y)q(y,x)}{\pi(x)q(x,y)}\right\}
\end{align}

This is exactly the standard Metropolis-Hastings acceptance probability! $\checkmark$

\subsubsection{Part (b): Baker algorithm}

For the Baker algorithm:
\begin{align}
\gamma(x,y) = \frac{\pi(x)q(x,y)\pi(y)q(y,x)}{\pi(x)q(x,y) + \pi(y)q(y,x)}
\end{align}

The acceptance probability is:
\begin{align}
\alpha(x,y) &= \frac{\gamma(x,y)}{\pi(x)q(x,y)}\\
&= \frac{\pi(x)q(x,y)\pi(y)q(y,x)}{\pi(x)q(x,y)[\pi(x)q(x,y) + \pi(y)q(y,x)]}\\
&= \frac{\pi(y)q(y,x)}{\pi(x)q(x,y) + \pi(y)q(y,x)}
\end{align}

\textbf{Verification of symmetry:}
\begin{align}
\gamma(y,x) &= \frac{\pi(y)q(y,x)\pi(x)q(x,y)}{\pi(y)q(y,x) + \pi(x)q(x,y)}\\
&= \frac{\pi(x)q(x,y)\pi(y)q(y,x)}{\pi(x)q(x,y) + \pi(y)q(y,x)} = \gamma(x,y)
\end{align}

Therefore, the Baker algorithm acceptance probability is:
\begin{align}
\boxed{\alpha(x,y) = \frac{\pi(y)q(y,x)}{\pi(x)q(x,y) + \pi(y)q(y,x)}}
\end{align}

\end{document}